{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math, argparse\n",
    "import torch, torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0702,  0.5333, -1.9662, -2.0433], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test CUDA\n",
    "torch.randn(4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "bptt_len = 30   # length of sequences for backpropagation through time \n",
    "clip = 0.25\n",
    "lr = 20\n",
    "log_interval = 100\n",
    "embedding_dim = 200\n",
    "num_layers = 2\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(lower = True, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make splits for data\n",
    "train, valid, test = torchtext.datasets.WikiText2.splits(TEXT, root = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2088628\n",
      "['<eos>', '=', 'valkyria', 'chronicles', 'iii', '=', '<eos>', '<eos>', 'senjō', 'no']\n",
      "<eos> = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 : <unk> chronicles ( japanese : 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan , is a tactical role @-@ playing video game developed by sega and media.vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the\n"
     ]
    }
   ],
   "source": [
    "print(len(train[0].text))\n",
    "print(train[0].text[:10])\n",
    "print(\" \".join(train[0].text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2088628, validation: 217646, test: 245569\n",
      "train: 2088620, validation: 217640, test: 245560\n"
     ]
    }
   ],
   "source": [
    "# trim the last extra words which form a small batch\n",
    "print(f\"train: {len(train[0].text)}, validation: {len(valid[0].text)}, test: {len(test[0].text)}\")\n",
    "train[0].text = train[0].text[:len(train[0].text)//batch_size*batch_size]\n",
    "valid[0].text = valid[0].text[:len(valid[0].text)//batch_size*batch_size]\n",
    "test[0].text  = test[0].text[ :len(test[0].text)// batch_size*batch_size]\n",
    "print(f\"train: {len(train[0].text)}, validation: {len(valid[0].text)}, test: {len(test[0].text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab): 28913\n",
      "The vocabulary contains all unique words and \"<pad>\": True\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab):', len(TEXT.vocab))\n",
    "print('The vocabulary contains all unique words and \"<pad>\":', set(TEXT.vocab.itos) == set(train[0].text + ['<pad>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, valid, test), batch_size = batch_size, bptt_len = bptt_len, device = 'cuda',repeat = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LSTM, input's shape is (sequence_length, batch_size): torch.Size([30, 20]) \n",
      "\n",
      "<eos> = homarus gammarus = <eos> <eos> homarus gammarus , known as the european lobster or common lobster , is a species of <unk> lobster from the eastern atlantic ocean , mediterranean sea and parts of the black sea . it is closely related to the american lobster , h. americanus . it may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . in life , the lobsters are blue , only becoming \" lobster red \" on cooking\n",
      "\n",
      "Each column is a sequence:\n",
      "<eos> = homarus gammarus = <eos> <eos> homarus gammarus , known as the european lobster or common lobster , is a species of <unk> lobster from the eastern atlantic ocean\n",
      ", mediterranean sea and parts of the black sea . it is closely related to the american lobster , h. americanus . it may grow to a length of 60\n",
      "\n",
      "The target contains next words:\n",
      "= homarus gammarus = <eos> <eos> homarus gammarus , known as the european lobster or common lobster , is a species of <unk> lobster from the eastern atlantic ocean ,\n",
      "mediterranean sea and parts of the black sea . it is closely related to the american lobster , h. americanus . it may grow to a length of 60 cm\n"
     ]
    }
   ],
   "source": [
    "I = iter(valid_iter)\n",
    "batch1 = next(I)\n",
    "batch2 = next(I)\n",
    "print(\"In LSTM, input's shape is (sequence_length, batch_size):\", batch1.text.shape, '\\n')\n",
    "print(' '.join(valid[0].text[:100]))\n",
    "print(\"\\nEach column is a sequence:\")\n",
    "print(' '.join([ TEXT.vocab.itos[i] for i in batch1.text[:, 0] ]))\n",
    "print(' '.join([ TEXT.vocab.itos[i] for i in batch2.text[:, 0] ]))\n",
    "print(\"\\nThe target contains next words:\")\n",
    "print(' '.join([ TEXT.vocab.itos[i] for i in batch1.target[:, 0] ]))\n",
    "print(' '.join([ TEXT.vocab.itos[i] for i in batch2.target[:, 0] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:      torch.Size([30, 20])\n",
      "embedding:  torch.Size([30, 20, 100])\n",
      "lstm:       torch.Size([30, 20, 100]) , hidden: 2*torch.Size([3, 20, 100])\n",
      "decode:     torch.Size([30, 20, 28913])\n"
     ]
    }
   ],
   "source": [
    "x = batch1.text.cpu()\n",
    "print(\"input:     \", x.shape)\n",
    "encoder = torch.nn.Embedding(len(TEXT.vocab), 100)\n",
    "x = encoder(x)\n",
    "print(\"embedding: \", x.shape)\n",
    "x, h = torch.nn.LSTM(100, 100, 3)(x)\n",
    "print(\"lstm:      \", x.shape, f\", hidden: {len(h)}*{h[0].shape}\")\n",
    "decoder = torch.nn.Linear(100, len(TEXT.vocab))  \n",
    "x = decoder(x)\n",
    "print(\"decode:    \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        hidden_size = embedding_dim  # because encoder & decoder share same weight \n",
    "        self.drop1 = torch.nn.Dropout(dropout)\n",
    "        self.encoder = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_size, num_layers, dropout = dropout)\n",
    "        self.drop2 = torch.nn.Dropout(dropout)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, num_embeddings)\n",
    "        self.decoder.weight = self.encoder.weight # tie weights\n",
    "        self.init_weights()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, input, hidden): \n",
    "        sequence_len, batch_size = input.shape\n",
    "        embedding = self.encoder(input)\n",
    "        embedding = self.drop1(embedding)\n",
    "        output, hidden = self.rnn(embedding, hidden)\n",
    "        output = self.drop2(output)\n",
    "        return self.decoder(output), hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_shape = (self.num_layers, batch_size, self.hidden_size)\n",
    "        weight1 = next(self.parameters()).data   # ensure same dtype and device\n",
    "        return (weight1.new_zeros(*hidden_shape), weight1.new_zeros(*hidden_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (drop1): Dropout(p=0.2)\n",
      "  (encoder): Embedding(28913, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
      "  (drop2): Dropout(p=0.2)\n",
      "  (decoder): Linear(in_features=200, out_features=28913, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lstm = RNNModel(len(TEXT.vocab), embedding_dim, num_layers, dropout).cuda()\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, epoch):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        input, target = batch.text, batch.target\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = [h.detach() for h in hidden]\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(input, hidden)\n",
    "        loss = criterion(output.view(-1, len(TEXT.vocab)), target.view(-1))\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters(): p.data.add_(-lr*p.grad.data)\n",
    "        if (i + 1) % log_interval == 0: print(f\"epoch {epoch:03d}, {i:4d}/{len(data_loader):4d} batches, lr: {lr:02.2f}, loss: {loss.item():5.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    loss = 0   \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:        \n",
    "            input, target = batch.text, batch.target\n",
    "            output, hidden = model(input, hidden)\n",
    "            loss += criterion(output.view(-1, len(TEXT.vocab)), target.view(-1))\n",
    "            hidden = [h.detach() for h in hidden]\n",
    "    return loss.item()/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,   99/3481 batches, lr: 20.00, loss: 7.177\n",
      "epoch 001,  199/3481 batches, lr: 20.00, loss: 6.662\n",
      "epoch 001,  299/3481 batches, lr: 20.00, loss: 6.512\n",
      "epoch 001,  399/3481 batches, lr: 20.00, loss: 6.184\n",
      "epoch 001,  499/3481 batches, lr: 20.00, loss: 6.153\n",
      "epoch 001,  599/3481 batches, lr: 20.00, loss: 6.177\n",
      "epoch 001,  699/3481 batches, lr: 20.00, loss: 6.039\n",
      "epoch 001,  799/3481 batches, lr: 20.00, loss: 6.132\n",
      "epoch 001,  899/3481 batches, lr: 20.00, loss: 6.154\n",
      "epoch 001,  999/3481 batches, lr: 20.00, loss: 5.780\n",
      "epoch 001, 1099/3481 batches, lr: 20.00, loss: 5.939\n",
      "epoch 001, 1199/3481 batches, lr: 20.00, loss: 6.003\n",
      "epoch 001, 1299/3481 batches, lr: 20.00, loss: 5.698\n",
      "epoch 001, 1399/3481 batches, lr: 20.00, loss: 5.548\n",
      "epoch 001, 1499/3481 batches, lr: 20.00, loss: 5.694\n",
      "epoch 001, 1599/3481 batches, lr: 20.00, loss: 5.468\n",
      "epoch 001, 1699/3481 batches, lr: 20.00, loss: 5.915\n",
      "epoch 001, 1799/3481 batches, lr: 20.00, loss: 6.010\n",
      "epoch 001, 1899/3481 batches, lr: 20.00, loss: 5.482\n",
      "epoch 001, 1999/3481 batches, lr: 20.00, loss: 5.723\n",
      "epoch 001, 2099/3481 batches, lr: 20.00, loss: 5.468\n",
      "epoch 001, 2199/3481 batches, lr: 20.00, loss: 5.752\n",
      "epoch 001, 2299/3481 batches, lr: 20.00, loss: 5.542\n",
      "epoch 001, 2399/3481 batches, lr: 20.00, loss: 5.649\n",
      "epoch 001, 2499/3481 batches, lr: 20.00, loss: 5.332\n",
      "epoch 001, 2599/3481 batches, lr: 20.00, loss: 5.631\n",
      "epoch 001, 2699/3481 batches, lr: 20.00, loss: 5.453\n",
      "epoch 001, 2799/3481 batches, lr: 20.00, loss: 5.300\n",
      "epoch 001, 2899/3481 batches, lr: 20.00, loss: 5.375\n",
      "epoch 001, 2999/3481 batches, lr: 20.00, loss: 5.427\n",
      "epoch 001, 3099/3481 batches, lr: 20.00, loss: 5.499\n",
      "epoch 001, 3199/3481 batches, lr: 20.00, loss: 4.743\n",
      "epoch 001, 3299/3481 batches, lr: 20.00, loss: 5.492\n",
      "epoch 001, 3399/3481 batches, lr: 20.00, loss: 5.239\n",
      "validation loss  5.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "epoch 002,   99/3481 batches, lr: 20.00, loss: 5.447\n",
      "epoch 002,  199/3481 batches, lr: 20.00, loss: 5.178\n",
      "epoch 002,  299/3481 batches, lr: 20.00, loss: 5.170\n",
      "epoch 002,  399/3481 batches, lr: 20.00, loss: 5.228\n",
      "epoch 002,  499/3481 batches, lr: 20.00, loss: 5.277\n",
      "epoch 002,  599/3481 batches, lr: 20.00, loss: 5.157\n",
      "epoch 002,  699/3481 batches, lr: 20.00, loss: 5.210\n",
      "epoch 002,  799/3481 batches, lr: 20.00, loss: 5.334\n",
      "epoch 002,  899/3481 batches, lr: 20.00, loss: 5.478\n",
      "epoch 002,  999/3481 batches, lr: 20.00, loss: 5.224\n",
      "epoch 002, 1099/3481 batches, lr: 20.00, loss: 5.188\n",
      "epoch 002, 1199/3481 batches, lr: 20.00, loss: 5.250\n",
      "epoch 002, 1299/3481 batches, lr: 20.00, loss: 5.142\n",
      "epoch 002, 1399/3481 batches, lr: 20.00, loss: 5.010\n",
      "epoch 002, 1499/3481 batches, lr: 20.00, loss: 5.316\n",
      "epoch 002, 1599/3481 batches, lr: 20.00, loss: 4.943\n",
      "epoch 002, 1699/3481 batches, lr: 20.00, loss: 5.488\n",
      "epoch 002, 1799/3481 batches, lr: 20.00, loss: 5.615\n",
      "epoch 002, 1899/3481 batches, lr: 20.00, loss: 5.046\n",
      "epoch 002, 1999/3481 batches, lr: 20.00, loss: 5.332\n",
      "epoch 002, 2099/3481 batches, lr: 20.00, loss: 5.027\n",
      "epoch 002, 2199/3481 batches, lr: 20.00, loss: 5.341\n",
      "epoch 002, 2299/3481 batches, lr: 20.00, loss: 5.067\n",
      "epoch 002, 2399/3481 batches, lr: 20.00, loss: 5.149\n",
      "epoch 002, 2499/3481 batches, lr: 20.00, loss: 4.923\n",
      "epoch 002, 2599/3481 batches, lr: 20.00, loss: 5.215\n",
      "epoch 002, 2699/3481 batches, lr: 20.00, loss: 5.052\n",
      "epoch 002, 2799/3481 batches, lr: 20.00, loss: 4.844\n",
      "epoch 002, 2899/3481 batches, lr: 20.00, loss: 5.074\n",
      "epoch 002, 2999/3481 batches, lr: 20.00, loss: 5.191\n",
      "epoch 002, 3099/3481 batches, lr: 20.00, loss: 5.135\n",
      "epoch 002, 3199/3481 batches, lr: 20.00, loss: 4.450\n",
      "epoch 002, 3299/3481 batches, lr: 20.00, loss: 5.118\n",
      "epoch 002, 3399/3481 batches, lr: 20.00, loss: 5.030\n",
      "validation loss  5.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Test loss  5.00\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for epoch in range(2):\n",
    "    train(lstm, train_iter, epoch + 1)\n",
    "    val_loss = evaluate(lstm, valid_iter)\n",
    "    print(f'validation loss {val_loss:5.2f}\\n' + '-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "\n",
    "print(f'\\n\\nTest loss {evaluate(lstm, test_iter):5.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eos> = <unk> ( <eos> <eos> the <unk> ( a american son in a , <unk> , , the was a <unk> appearance time <unk> in the <unk> series ,\n",
      "\n",
      "@-@ century . , and the <unk> were also with the . years . which the longer was <unk> by <eos> <unk> <unk> of was <unk> also by the ,\n",
      "\n",
      ". <eos> , the was a first in in of and , , to <unk> . who the of a <unk> state of <eos> was first was @-@ also @.@\n",
      "\n",
      "than , % year ( , , the <unk> the <unk> of the up game to , the <unk> were <unk> <unk> was up and to the <unk> <unk> .\n",
      "\n",
      ", <eos> <unk> of the was <unk> the the <unk> , <unk> , , was also of the <unk> <unk> . . by be a , the . the the\n",
      "\n",
      "<unk> @-@ . the . states <eos> <eos> = = = <unk> = the = = = = <eos> <eos> the the december 2006 , was a @-@ , ,\n",
      "\n",
      "<unk> . . the <eos> first of the <unk> of the the of the of starlings <unk> schools . and the to the <unk> . and <unk> <unk> , and\n",
      "\n",
      ", <unk> @-@ . the so . the was the the one on first year , the example first time , the was announced by the <unk> <eos> game was\n",
      "\n",
      "the in . , 'malley . a <unk> <unk> @-@ <unk> <unk> a was in in a . . . and the also most <unk> <unk> to . the .\n",
      "\n",
      "% and . and york . , , the , be be to the the <unk> . . . to the <unk> \" \" . \" a a to a\n",
      "\n",
      "was the difficult to the . death <unk> \" . . the , was of <unk> called the by the <unk> of the <unk> <unk> , the <unk> to the\n",
      "\n",
      "<unk> francisco <unk> . the 's were as <unk> , <unk> , <unk> , the <unk> . that the <unk> <unk> the <unk> of of <unk> be the <unk> .\n",
      "\n",
      ", the <unk> – season . . <unk> . the <unk> @-@ 10 lead . the the 10 , the , the was a number with to the <eos> =\n",
      "\n",
      "<unk> the same . the . and the first was were to the <unk> . <unk> . the was who , <unk> of in the in than two first time\n",
      "\n",
      "the to the . and the was also by after the death . <eos> <eos> , in the to be been a , the , the % the was be\n",
      "\n",
      "and the , and , and <unk> . . the <unk> of <unk> <unk> was a of the most successful and characters and . the united of <eos> <eos> first\n",
      "\n",
      "<unk> to to to be the the the is no from the two and , the , , <unk> <unk> <unk> the <unk> , as <unk> , , the the\n",
      "\n",
      ". the <unk> of of 4 @.@ . the the first , to a <eos> , <unk> , <unk> union . and , and <unk> . <eos> of , ,\n",
      "\n",
      "the to the <eos> = = the <unk> = <eos> <eos> the <unk> ( died <unk> ) 2006 ) is a american son in , , a @.@ 4 in\n",
      "\n",
      "and the of , the the , the the were , the , the <unk> of be be the to be . and that the the <unk> of <unk> to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "hidden = lstm.init_hidden(batch_size)\n",
    "input = next(iter(test_iter)).text\n",
    "with torch.no_grad(): output = torch.argmax( lstm(input, hidden)[0], dim = 2)\n",
    "\n",
    "sentence = [[]]* batch_size\n",
    "for j in range(batch_size): sentence[j] = [TEXT.vocab.itos[i] for i in output[:, j]]\n",
    "_  = [print(' '.join(s) + '\\n') for s in sentence]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
