{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/PacktPublishing/Deep-Learning-with-PyTorch/tree/master/Chapter06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "3.7.3 (default, Apr  3 2019, 05:39:12) \n",
      "[GCC 8.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embedding by building a sentiment classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT  = torchtext.data.Field(lower = True, batch_first = True, fix_length = 40)\n",
    "LABEL = torchtext.data.Field(sequential = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL, root = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.datasets.imdb.IMDB"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7f5898059198>, 'label': <torchtext.data.field.Field object at 0x7f5898059860>}\n",
      "len(train) 25000\n",
      "vars(train[0]) {'text': ['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"teachers\".', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"teachers\".', 'the', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'i', 'immediately', 'recalled', '.........', 'at', '..........', 'high.', 'a', 'classic', 'line:', 'inspector:', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'student:', 'welcome', 'to', 'bromwell', 'high.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched.', 'what', 'a', 'pity', 'that', 'it', \"isn't!\"], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in each text varies, such as [140, 428, 147, 124, 120]\n",
      "But TEXT.fix_length tells iterator (loader) to only first 40 words.\n"
     ]
    }
   ],
   "source": [
    "# Zhuoer's Note\n",
    "print(\"Number of words in each text varies, such as\", [len(train[i].text) for i in range(5)])\n",
    "print(\"But TEXT.fix_length tells iterator (loader) to only first\", TEXT.fix_length, \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_vectors = torchtext.vocab.GloVe(name = '6B', dim = 300, cache = \"data/vocab\")\n",
    "TEXT.build_vocab(train, vectors = vocab_vectors, max_size = 10000, min_freq = 10)\n",
    "LABEL.build_vocab(train)\n",
    "# note train and test share the same TEXT and LABEL fields, so we can creater iterator for both below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pos': 12500, 'neg': 12500})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'itos', 'stoi', 'vectors'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(TEXT.vocab).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.7724, -0.1800,  0.2072,  ...,  0.6736,  0.2263, -0.2919],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "torch.Size([10002, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)\n",
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe: 400000 400000 torch.Size([400000, 300]) 300\n",
      "max_size + 2(unk, pad), 10002\n"
     ]
    }
   ],
   "source": [
    "# Zhuoer's Note\n",
    "print(\"GloVe:\", len(vocab_vectors.itos), len(vocab_vectors.stoi), vocab_vectors.vectors.shape, vocab_vectors.dim)\n",
    "print(\"max_size + 2(unk, pad),\", len(TEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frist dataset is assumed to be train, and `shuffle`, `sort` is set accordingly\n",
    "train_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, test), batch_sizes = (32, 128),  device = \"cpu\", repeat = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size:  10002\n",
      "Words not in the vocabulary gets ID 0, and is saved in stoi, so the latter's size grows.\n",
      "Load a batch:  10147\n"
     ]
    }
   ],
   "source": [
    "# Zhuoer's Note\n",
    "print(\"initial size: \", len(TEXT.vocab.stoi))\n",
    "next(iter(train_iter))\n",
    "print(\"Words not in the vocabulary gets ID 0, and is saved in stoi, so the latter's size grows.\")\n",
    "print(\"Load a batch: \", len(TEXT.vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sentences:\t bromwell high is a cartoon comedy. it ran at the same time as some other programs about school life, such as \"teachers\". my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is \"teachers\". the scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled ......... at .......... high. a classic line: inspector: i'm here to sack one of your teachers. student: welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! \n",
      "\n",
      "First 40 (see fix_length above) words are converted to indexs: \n",
      "\t tensor([[   0,  317,    7,    3, 1309, 1299,   12, 2166,   29,    2,  163,   84,\n",
      "           15,   45,   80, 8819,   43,  466,  747,  130,   15,    0,   57, 7010,\n",
      "          181,    8,    2, 5591, 8659,  475,   87,    6,  250,   11,    0,    0,\n",
      "         2559,    7,   76, 2532]]) \n",
      "\n",
      "And we can convert it back:\t ['<unk>', 'high', 'is', 'a', 'cartoon', 'comedy.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '<unk>', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', '<unk>', '<unk>', 'satire', 'is', 'much', 'closer']\n"
     ]
    }
   ],
   "source": [
    "# Zhuoer's Note\n",
    "review_sample = ' '.join(train[0].text)\n",
    "    # create an Example from scratch\n",
    "example = torchtext.data.Example()\n",
    "example.text = review_sample.split(' ')\n",
    "example.label = \"pos\"\n",
    "    # create dataset of single item\n",
    "train2 = torchtext.data.Dataset([example],[('text', TEXT),('label', LABEL)])\n",
    "test2  = torchtext.data.Dataset([test[0]],[('text', TEXT),('label', LABEL)])\n",
    "\n",
    "train_iter2, test_iter2 = torchtext.data.BucketIterator.splits(\n",
    "    (train2,test2), batch_sizes = (1, 1),  device = \"cpu\", repeat = False\n",
    ")\n",
    "\n",
    "print(\"Raw sentences:\\t\", review_sample, '\\n')\n",
    "print(\"First 40 (see fix_length above) words are converted to indexs: \\n\\t\", next(iter(train_iter2)).text, '\\n')\n",
    "print(\"And we can convert it back:\\t\", [TEXT.vocab.itos[i] for i in next(iter(train_iter2)).text.numpy()[0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this model, `torchtext.vocab.GloVe()` only provides a vocabulary, it builds embedding on it own.\n",
    "# The vocabulary gives most frequent words integer IDs, while other words (such as \"bromwell\" in above) is treated as unknown with ID 0\n",
    "class EmbNet(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, input_len):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings,embedding_dim)\n",
    "        self.fc = torch.nn.Linear(embedding_dim*input_len, 3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embeds = self.embedding(x).view(x.size(0),-1)\n",
    "        out = self.fc(embeds)\n",
    "        return torch.nn.functional.log_softmax(out, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbNet(len(TEXT.vocab), 10, TEXT.fix_length)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epoch, model, data_loader, optimizer, phase = 'training'):\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if phase == 'validation':\n",
    "        model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for batch_idx , batch in enumerate(data_loader):\n",
    "        text , target = batch.text, batch.label\n",
    "        text,target = text.cuda(),target.cuda()\n",
    "        \n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = torch.nn.functional.nll_loss(output,target)\n",
    "        \n",
    "        running_loss += torch.nn.functional.nll_loss(output, target, reduction = \"sum\").data.item()\n",
    "        preds = torch.argmax(output, dim = 1)\n",
    "        running_correct += torch.sum(preds == target)\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = float(running_correct)/len(data_loader.dataset)\n",
    "    \n",
    "    print(f'{phase:>10}: loss is {loss:4.2f} and {phase:>10} accuracy is {running_correct}/{len(data_loader.dataset)} ({accuracy:.2%})')\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training: loss is 0.74 and   training accuracy is 13333/25000 (53.33%)\n",
      "validation: loss is 0.70 and validation accuracy is 13738/25000 (54.95%)\n",
      "  training: loss is 0.68 and   training accuracy is 14605/25000 (58.42%)\n",
      "validation: loss is 0.68 and validation accuracy is 14769/25000 (59.08%)\n",
      "  training: loss is 0.64 and   training accuracy is 15798/25000 (63.19%)\n",
      "validation: loss is 0.64 and validation accuracy is 15705/25000 (62.82%)\n",
      "  training: loss is 0.60 and   training accuracy is 16945/25000 (67.78%)\n",
      "validation: loss is 0.62 and validation accuracy is 16526/25000 (66.10%)\n",
      "  training: loss is 0.56 and   training accuracy is 17880/25000 (71.52%)\n",
      "validation: loss is 0.61 and validation accuracy is 16813/25000 (67.25%)\n",
      "  training: loss is 0.52 and   training accuracy is 18555/25000 (74.22%)\n",
      "validation: loss is 0.60 and validation accuracy is 17222/25000 (68.89%)\n",
      "  training: loss is 0.49 and   training accuracy is 19051/25000 (76.20%)\n",
      "validation: loss is 0.60 and validation accuracy is 17382/25000 (69.53%)\n",
      "  training: loss is 0.46 and   training accuracy is 19479/25000 (77.92%)\n",
      "validation: loss is 0.60 and validation accuracy is 17546/25000 (70.18%)\n",
      "  training: loss is 0.43 and   training accuracy is 19899/25000 (79.60%)\n",
      "validation: loss is 0.60 and validation accuracy is 17583/25000 (70.33%)\n",
      "  training: loss is 0.41 and   training accuracy is 20261/25000 (81.04%)\n",
      "validation: loss is 0.61 and validation accuracy is 17638/25000 (70.55%)\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracy = [],[]\n",
    "val_losses, val_accuracy = [],[]\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, train_iter, optimizer, phase = 'training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch, model, test_iter, optimizer, phase = 'validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 40])\n",
      "torch.Size([32, 40, 10])\n"
     ]
    }
   ],
   "source": [
    "# Zhuoer's Note\n",
    "print(next(iter(train_iter)).text.shape)\n",
    "print(model.embedding(next(iter(train_iter)).text.cuda()).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbNet2(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, input_len):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.fc = torch.nn.Linear(embedding_dim*input_len, 3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embeds = self.embedding(x).view(x.size(0),-1)\n",
    "        out = self.fc(embeds)\n",
    "        return torch.nn.functional.log_softmax(out, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbNet2(\n",
       "  (embedding): Embedding(10002, 300)\n",
       "  (fc): Linear(in_features=12000, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = EmbNet2(TEXT.vocab.vectors.shape[0], TEXT.vocab.vectors.shape[1], TEXT.fix_length)\n",
    "model2 = model2.cuda()\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.embedding.weight.data = TEXT.vocab.vectors.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.SGD(model2.fc.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training: loss is 0.69 and   training accuracy is 14329/25000 (57.32%)\n",
      "validation: loss is 0.66 and validation accuracy is 15298/25000 (61.19%)\n",
      "  training: loss is 0.65 and   training accuracy is 16020/25000 (64.08%)\n",
      "validation: loss is 0.65 and validation accuracy is 15855/25000 (63.42%)\n",
      "  training: loss is 0.62 and   training accuracy is 16659/25000 (66.64%)\n",
      "validation: loss is 0.64 and validation accuracy is 16106/25000 (64.42%)\n",
      "  training: loss is 0.61 and   training accuracy is 17179/25000 (68.72%)\n",
      "validation: loss is 0.63 and validation accuracy is 16257/25000 (65.03%)\n",
      "  training: loss is 0.60 and   training accuracy is 17444/25000 (69.78%)\n",
      "validation: loss is 0.62 and validation accuracy is 16419/25000 (65.68%)\n",
      "  training: loss is 0.59 and   training accuracy is 17643/25000 (70.57%)\n",
      "validation: loss is 0.62 and validation accuracy is 16556/25000 (66.22%)\n",
      "  training: loss is 0.58 and   training accuracy is 17821/25000 (71.28%)\n",
      "validation: loss is 0.62 and validation accuracy is 16611/25000 (66.44%)\n",
      "  training: loss is 0.57 and   training accuracy is 18014/25000 (72.06%)\n",
      "validation: loss is 0.61 and validation accuracy is 16644/25000 (66.58%)\n",
      "  training: loss is 0.56 and   training accuracy is 18133/25000 (72.53%)\n",
      "validation: loss is 0.61 and validation accuracy is 16692/25000 (66.77%)\n",
      "  training: loss is 0.56 and   training accuracy is 18216/25000 (72.86%)\n",
      "validation: loss is 0.61 and validation accuracy is 16690/25000 (66.76%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    fit(epoch, model2, train_iter, optimizer2, phase='training')\n",
    "    fit(epoch, model2, test_iter, optimizer2, phase='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT3  = torchtext.data.Field(lower = True, batch_first = False, fix_length = 200)\n",
    "\n",
    "train3, test3 = torchtext.datasets.IMDB.splits(TEXT3, LABEL, root = \"data\")\n",
    "\n",
    "TEXT3.build_vocab(train3, vectors = vocab_vectors, max_size = 10000, min_freq = 10)\n",
    "\n",
    "train_iter3, test_iter3 = torchtext.data.BucketIterator.splits(\n",
    "    (train3, test3), batch_size = 32, device = \"cpu\", repeat = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBRnn(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, num_classes, batch_size, num_layers = 2):\n",
    "        super().__init__()\n",
    "        self.e = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, embedding_dim, num_layers)\n",
    "        self.dropout = torch.nn.Dropout(p = 0.8)\n",
    "        self.fc = torch.nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.e(input)\n",
    "        input, _ = self.lstm(input) \n",
    "        input = self.dropout(input[-1]) # use the output of last layer\n",
    "        input = self.fc(input)\n",
    "        return torch.nn.functional.log_softmax(input, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = IMDBRnn(len(TEXT3.vocab), 100, 3, 32)\n",
    "model3 = model3.cuda()\n",
    "\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([200, 32])\n",
      "output: torch.Size([32, 3])\n",
      "embeding: torch.Size([200, 32, 100])\n",
      "LSTM: torch.Size([200, 32, 100])\n",
      "Linear: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "input = next(iter(train_iter3)).text\n",
    "print(\"input:\", input.shape)\n",
    "print(\"output:\", model3(input.cuda()).shape)\n",
    "input = torch.nn.Embedding(len(TEXT3.vocab), 100)(input)\n",
    "print(\"embeding:\", input.shape)\n",
    "input, _ = torch.nn.LSTM(100, 100, 2)(input)\n",
    "print(\"LSTM:\", input.shape)\n",
    "input = torch.nn.Linear(100, 3)(input[-1])\n",
    "print(\"Linear:\", input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training: loss is 0.71 and   training accuracy is 12492/25000 (49.97%)\n",
      "validation: loss is 0.69 and validation accuracy is 12500/25000 (50.00%)\n",
      "  training: loss is 0.70 and   training accuracy is 12586/25000 (50.34%)\n",
      "validation: loss is 0.70 and validation accuracy is 12500/25000 (50.00%)\n",
      "  training: loss is 0.70 and   training accuracy is 12488/25000 (49.95%)\n",
      "validation: loss is 0.69 and validation accuracy is 12504/25000 (50.02%)\n",
      "  training: loss is 0.69 and   training accuracy is 12768/25000 (51.07%)\n",
      "validation: loss is 0.69 and validation accuracy is 12500/25000 (50.00%)\n",
      "  training: loss is 0.69 and   training accuracy is 13574/25000 (54.30%)\n",
      "validation: loss is 0.71 and validation accuracy is 14164/25000 (56.66%)\n",
      "  training: loss is 0.56 and   training accuracy is 18336/25000 (73.34%)\n",
      "validation: loss is 0.50 and validation accuracy is 19307/25000 (77.23%)\n",
      "  training: loss is 0.43 and   training accuracy is 20433/25000 (81.73%)\n",
      "validation: loss is 0.45 and validation accuracy is 19892/25000 (79.57%)\n",
      "  training: loss is 0.37 and   training accuracy is 21202/25000 (84.81%)\n",
      "validation: loss is 0.42 and validation accuracy is 20433/25000 (81.73%)\n",
      "  training: loss is 0.31 and   training accuracy is 21997/25000 (87.99%)\n",
      "validation: loss is 0.40 and validation accuracy is 20584/25000 (82.34%)\n",
      "  training: loss is 0.26 and   training accuracy is 22617/25000 (90.47%)\n",
      "validation: loss is 0.44 and validation accuracy is 20398/25000 (81.59%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    fit(epoch, model3, train_iter3, optimizer3, phase = 'training')\n",
    "    fit(epoch, model3, test_iter3,  optimizer3, phase = 'validation')\n",
    "    # after ~5 epochs, the model overfit since validation loss increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional network on sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT4  = torchtext.data.Field(lower = True, batch_first = True, fix_length = 200)\n",
    "\n",
    "train4, test4 = torchtext.datasets.IMDB.splits(TEXT4, LABEL, root = \"data\")\n",
    "\n",
    "TEXT4.build_vocab(train4, vectors = vocab_vectors, max_size = 10000, min_freq = 10)\n",
    "\n",
    "train_iter4, test_iter4 = torchtext.data.BucketIterator.splits(\n",
    "    (train4, test4), batch_size = 32, device = \"cpu\", repeat = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBCovn(torch.nn.Module):\n",
    "    # the original code doesn't transpose(), rather uses Conv1d(fix_length, 100, 3), the result looks similar (or even better)\n",
    "    # but I think in_channels should be length of embedding vector, rather than number of words\n",
    "    def __init__(self, num_embeddings, embedding_dim, num_classes, batch_size, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        out_channels = embedding_dim\n",
    "        self.e = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.conv = torch.nn.Conv1d(embedding_dim, out_channels, kernel_size)\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool1d(10)\n",
    "        self.dropout = torch.nn.Dropout(p = 0.95)\n",
    "        self.fc = torch.nn.Linear(out_channels*10, num_classes)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.e(input)\n",
    "        input = self.conv(input.transpose(1, 2)) \n",
    "        input = self.avgpool(input)\n",
    "        input = self.dropout(input.view(input.shape[0], -1))\n",
    "        input = self.fc(input)\n",
    "        return self.softmax(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = IMDBCovn(len(TEXT4.vocab), 100, 3, 32)\n",
    "model4 = model4.cuda()\n",
    "\n",
    "optimizer4 = torch.optim.Adam(model4.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([32, 200])\n",
      "output: torch.Size([32, 3])\n",
      "embeding: torch.Size([32, 200, 100])\n",
      "Conv1d: torch.Size([32, 100, 198])\n",
      "Pool: torch.Size([32, 100, 10])\n"
     ]
    }
   ],
   "source": [
    "input = next(iter(train_iter4)).text\n",
    "print(\"input:\", input.shape)\n",
    "print(\"output:\", model4(input.cuda()).shape)\n",
    "input = torch.nn.Embedding(len(TEXT4.vocab), 100)(input)\n",
    "print(\"embeding:\", input.shape)\n",
    "input = torch.nn.Conv1d(100, 100, 3)(input.transpose(1, 2))\n",
    "print(\"Conv1d:\", input.shape)\n",
    "input = torch.nn.AdaptiveAvgPool1d(10)(input)\n",
    "print(\"Pool:\", input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training: loss is 0.77 and   training accuracy is 13450/25000 (53.80%)\n",
      "validation: loss is 0.61 and validation accuracy is 16996/25000 (67.98%)\n",
      "  training: loss is 0.62 and   training accuracy is 16922/25000 (67.69%)\n",
      "validation: loss is 0.47 and validation accuracy is 19373/25000 (77.49%)\n",
      "  training: loss is 0.50 and   training accuracy is 19197/25000 (76.79%)\n",
      "validation: loss is 0.41 and validation accuracy is 20286/25000 (81.14%)\n",
      "  training: loss is 0.44 and   training accuracy is 20201/25000 (80.80%)\n",
      "validation: loss is 0.39 and validation accuracy is 20512/25000 (82.05%)\n",
      "  training: loss is 0.39 and   training accuracy is 20881/25000 (83.52%)\n",
      "validation: loss is 0.36 and validation accuracy is 20987/25000 (83.95%)\n",
      "  training: loss is 0.36 and   training accuracy is 21202/25000 (84.81%)\n",
      "validation: loss is 0.37 and validation accuracy is 20943/25000 (83.77%)\n",
      "  training: loss is 0.34 and   training accuracy is 21470/25000 (85.88%)\n",
      "validation: loss is 0.37 and validation accuracy is 21071/25000 (84.28%)\n",
      "  training: loss is 0.32 and   training accuracy is 21715/25000 (86.86%)\n",
      "validation: loss is 0.38 and validation accuracy is 20998/25000 (83.99%)\n",
      "  training: loss is 0.30 and   training accuracy is 21964/25000 (87.86%)\n",
      "validation: loss is 0.41 and validation accuracy is 20928/25000 (83.71%)\n",
      "  training: loss is 0.29 and   training accuracy is 22063/25000 (88.25%)\n",
      "validation: loss is 0.41 and validation accuracy is 20959/25000 (83.84%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    fit(epoch, model4, train_iter4, optimizer4, phase = 'training')\n",
    "    fit(epoch, model4, test_iter4,  optimizer4, phase = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
